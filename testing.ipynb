{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T15:00:09.877330Z",
     "start_time": "2021-03-08T15:00:09.634716Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816496580927726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/memesrized/.virtualenvs/edu/lib/python3.8/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "hypothesis = [\"open\", \"the\", \"file\"]\n",
    "reference = [\"open\", \"file\"]\n",
    "# the maximum is bigram, so assign the weight into 2 half.\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = (0.5, 0.5))\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Rogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "def get_rouge(titles, generated_titles):\n",
    "    rouge = Rouge()\n",
    "    preprocessed = lambda sentences: [sent.strip().lower() for sent in sentences]\n",
    "    rouge_scores =  rouge.get_scores(preprocessed(titles), preprocessed(generated_titles), avg=True)\n",
    "    return {k: v['f'] for k, v in rouge_scores.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T20:54:41.355095Z",
     "start_time": "2021-03-03T20:54:41.348485Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T21:09:30.136872Z",
     "start_time": "2021-03-03T21:09:30.134051Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T21:09:30.738147Z",
     "start_time": "2021-03-03T21:09:30.730953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T21:09:33.799981Z",
     "start_time": "2021-03-03T21:09:33.788007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T11:57:06.816944Z",
     "start_time": "2021-03-08T11:57:04.489069Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers in /home/memesrized/.local/lib/python3.8/site-packages (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /home/memesrized/.local/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /home/memesrized/.local/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/memesrized/.local/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /home/memesrized/.local/lib/python3.8/site-packages (from transformers) (4.56.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/memesrized/.local/lib/python3.8/site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /home/memesrized/.local/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/memesrized/.local/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/memesrized/.local/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/lib/python3/dist-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/memesrized/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/memesrized/.local/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T11:56:48.562734Z",
     "start_time": "2021-03-08T11:56:47.357178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.3.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T12:03:08.853572Z",
     "start_time": "2021-03-08T12:02:54.899237Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "import torch\n",
    "\n",
    "folder = \"models/gpt2/v280/\"\n",
    "model = GPT2LMHeadModel.from_pretrained(folder)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T12:08:15.507219Z",
     "start_time": "2021-03-08T12:08:15.503680Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T12:08:15.610621Z",
     "start_time": "2021-03-08T12:08:15.603165Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T12:14:23.526386Z",
     "start_time": "2021-03-08T12:14:18.913496Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Why are a lot of people fucking morons?:paraphrase:Why are you so busy being stupid?\n",
      "\n",
      "\n",
      "1: Why are a lot of people fucking morons?:paraphrase:What is so funny about all the people who are stupid?\n",
      "\n",
      "\n",
      "2: Why are a lot of people fucking morons?:paraphrase:Are there many people being morons?\n",
      "\n",
      "\n",
      "3: Why are a lot of people fucking morons?:paraphrase:What is the most common reason we're so poorlyeducated?\n",
      "\n",
      "\n",
      "4: Why are a lot of people fucking morons?:paraphrase:Why are people who are very gullible and stupid not having any reason to believe what they are told?\n",
      "\n",
      "\n",
      "5: Why are a lot of people fucking morons?:paraphrase:Do all people, except some morons, believe the Earth is flat?\n",
      "\n",
      "\n",
      "6: Why are a lot of people fucking morons?:paraphrase:Why do so many people use Quora to answer questions?\n",
      "\n",
      "\n",
      "7: Why are a lot of people fucking morons?:paraphrase:What is wrong with people that are so stupid?\n",
      "\n",
      "\n",
      "8: Why are a lot of people fucking morons?:paraphrase:Are all people ignorant?\n",
      "\n",
      "\n",
      "9: Why are a lot of people fucking morons?:paraphrase:Why are people so mean and stupid?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Why are a lot of people fucking morons?:paraphrase:\"\n",
    "#prompt = \"<|startoftext|> From the 18th through the late 20th century, all the history of science, especially of the physical and biological sciences, was often presented as a progressive accumulation of knowledge. <|sep|>\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                do_sample=True,   \n",
    "                                top_k=50, \n",
    "                                max_length = 300,\n",
    "                                top_p=0.95, \n",
    "                                num_return_sequences=10\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu-kernel",
   "language": "python",
   "name": "edu-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
